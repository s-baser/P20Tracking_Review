{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# <b>(Phase 2) Screening: Screening the results of the ERIC API results using Python for a Systematic Review of P-20 Tracking</b>\n",
    "---\n",
    "This Jupyter Notebook imports the articles from the ERIC API Search (Searches 1-4) and conducts the first round of screening. It uses a by-hand journal screening file that is availble in the github project.\n",
    "\n",
    "<ul>\n",
    "    <li> <b>Language Screening (Non-English)</b></li>\n",
    "    <li> <b>Geography Code Screening (Non-US contexts)</b></li> \n",
    "    <li> <b>Publication Type</b></li>\n",
    "    <li> <b>Journal Screen (By Hand)</b></li>   \n",
    "    <li> <b>Less Relevant Subjects Screen</b></li>\n",
    "    <li> <b>Publication Type Screen (#2)</b></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<b>Project:</b> Systematic Literature Review of Tracking in P-20 Education<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Import the CSV from the ERIC API Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Load\n",
      "Total records loaded: 10272\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source_search</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publicationdateyear</th>\n",
       "      <th>...</th>\n",
       "      <th>institution</th>\n",
       "      <th>iesfunded</th>\n",
       "      <th>ieswwcreviewed</th>\n",
       "      <th>ieslinkwwcreviewguide</th>\n",
       "      <th>isbn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EJ061632</td>\n",
       "      <td>Search 1</td>\n",
       "      <td>Use of a \"Balance-Sheet\" Procedure to Improve ...</td>\n",
       "      <td>Mann, Leon</td>\n",
       "      <td>1972</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EJ203027</td>\n",
       "      <td>Search 1</td>\n",
       "      <td>A Procedure to Estimate the Probability of Err...</td>\n",
       "      <td>Ackerson, Gary E., And Others</td>\n",
       "      <td>1978</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EJ201539</td>\n",
       "      <td>Search 1</td>\n",
       "      <td>Entwicklung eines Einstufungstests fuer Deutsc...</td>\n",
       "      <td>Kummer, Manfred, And Others</td>\n",
       "      <td>1978</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EJ196127</td>\n",
       "      <td>Search 1</td>\n",
       "      <td>The Principal and Special Education Placement.</td>\n",
       "      <td>Yoshida, Roland K., And Others</td>\n",
       "      <td>1978</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EJ199000</td>\n",
       "      <td>Search 1</td>\n",
       "      <td>Curriculum Tracking and Educational Stratifica...</td>\n",
       "      <td>Alexander, Karl L., And Others</td>\n",
       "      <td>1978</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10267</th>\n",
       "      <td>EJ1431054</td>\n",
       "      <td>Search 2</td>\n",
       "      <td>Announcing New NHSDA Programming Connecting Se...</td>\n",
       "      <td>Susan McGreevy-Nichols, Marissa Finkelstein</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10268</th>\n",
       "      <td>EJ1426660</td>\n",
       "      <td>Search 3</td>\n",
       "      <td>Student Perceptions of Glover/Curwen Hand Sign...</td>\n",
       "      <td>Whitney Mayo</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10269</th>\n",
       "      <td>EJ1418576</td>\n",
       "      <td>Search 3</td>\n",
       "      <td>Development of Comprehension Monitoring Skill ...</td>\n",
       "      <td>Kunyu Xu, Yu-Min Ku, Chenlu Ma, Chien-Hui Lin,...</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10270</th>\n",
       "      <td>EJ1407862</td>\n",
       "      <td>Search 2</td>\n",
       "      <td>Is &amp;quot;Option B&amp;quot; a Viable Plan B? Schoo...</td>\n",
       "      <td>Amy E. Stich, George Spencer, Brionna Johnson,...</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10271</th>\n",
       "      <td>EJ1425303</td>\n",
       "      <td>Search 2</td>\n",
       "      <td>Pipeline Disruption: The Impact of COVID-19 on...</td>\n",
       "      <td>Aubrey Scheopner Torres, Lisa Andries D'Souza</td>\n",
       "      <td>2024</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10272 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id source_search  \\\n",
       "0       EJ061632      Search 1   \n",
       "1       EJ203027      Search 1   \n",
       "2       EJ201539      Search 1   \n",
       "3       EJ196127      Search 1   \n",
       "4       EJ199000      Search 1   \n",
       "...          ...           ...   \n",
       "10267  EJ1431054      Search 2   \n",
       "10268  EJ1426660      Search 3   \n",
       "10269  EJ1418576      Search 3   \n",
       "10270  EJ1407862      Search 2   \n",
       "10271  EJ1425303      Search 2   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Use of a \"Balance-Sheet\" Procedure to Improve ...   \n",
       "1      A Procedure to Estimate the Probability of Err...   \n",
       "2      Entwicklung eines Einstufungstests fuer Deutsc...   \n",
       "3         The Principal and Special Education Placement.   \n",
       "4      Curriculum Tracking and Educational Stratifica...   \n",
       "...                                                  ...   \n",
       "10267  Announcing New NHSDA Programming Connecting Se...   \n",
       "10268  Student Perceptions of Glover/Curwen Hand Sign...   \n",
       "10269  Development of Comprehension Monitoring Skill ...   \n",
       "10270  Is &quot;Option B&quot; a Viable Plan B? Schoo...   \n",
       "10271  Pipeline Disruption: The Impact of COVID-19 on...   \n",
       "\n",
       "                                                  author  publicationdateyear  \\\n",
       "0                                             Mann, Leon                 1972   \n",
       "1                          Ackerson, Gary E., And Others                 1978   \n",
       "2                            Kummer, Manfred, And Others                 1978   \n",
       "3                         Yoshida, Roland K., And Others                 1978   \n",
       "4                         Alexander, Karl L., And Others                 1978   \n",
       "...                                                  ...                  ...   \n",
       "10267        Susan McGreevy-Nichols, Marissa Finkelstein                 2024   \n",
       "10268                                       Whitney Mayo                 2024   \n",
       "10269  Kunyu Xu, Yu-Min Ku, Chenlu Ma, Chien-Hui Lin,...                 2024   \n",
       "10270  Amy E. Stich, George Spencer, Brionna Johnson,...                 2024   \n",
       "10271      Aubrey Scheopner Torres, Lisa Andries D'Souza                 2024   \n",
       "\n",
       "       ... institution iesfunded ieswwcreviewed ieslinkwwcreviewguide isbn  \n",
       "0      ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "1      ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "2      ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "3      ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "4      ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "...    ...         ...       ...            ...                   ...  ...  \n",
       "10267  ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "10268  ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "10269  ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "10270  ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "10271  ...         NaN       NaN            NaN                   NaN  NaN  \n",
       "\n",
       "[10272 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load data from specified file path\n",
    "file_path = r\"C:\\Users\\sbaser\\OneDrive - University of Georgia\\Shared Folders\\P-20 Parallels and Perils\\Data Collection\\Phase 2 - Literature Review Search\\ERIC API Resources\\ERIC Searches\\Phase 2 Searches\\Tracking_API_Output_Phase_2b_Search 1.csv\"\n",
    "master_records = pd.read_csv(file_path)\n",
    "\n",
    "# Display total records loaded\n",
    "print(\"Initial Data Load\")\n",
    "print(f\"Total records loaded: {master_records.shape[0]}\")\n",
    "\n",
    "# Set display options for scrollable view if the DataFrame is large\n",
    "pd.options.display.max_rows = 10  # Adjust as needed for number of rows to display at once\n",
    "pd.options.display.max_columns = 10  # Adjust as needed for number of columns to display at once\n",
    "\n",
    "# Display the DataFrame\n",
    "display(master_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <ins>Screening the Results for Relevant P-20 Tracking Articles</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screening 1: Screen based on language (Non-English) & geography code (Non-US contexts)\n",
    "\n",
    "This section of the Jupyter notebook performs the initial screening of articles based on two criteria: language and geography code. The purpose of this screening is to filter out articles that are not relevant to the scope of the study.\n",
    "\n",
    "<b>a. Language Screening (Non-English):</b>\n",
    "<ul>\n",
    "    <li>Articles are first screened based on their language metadata. Only articles that are in English or have missing language information are retained. Non-English articles are identified and removed during this step.</li>\n",
    "    <li>The count of articles for each language is displayed both before and after the screening to ensure transparency and accuracy in the filtering process.</li>\n",
    "</ul>\n",
    "\n",
    "<b>b. Geography Code Screening (Non-US contexts):</b>\n",
    "<ul>\n",
    "    <li>Following the language screening, articles are further filtered based on their geography code. The geography code screening is aimed at identifying and removing articles that pertain to non-US contexts, as determined through a preliminary analysis in Excel.</li>\n",
    "    <li>The list of non-US geographical identifiers, along with the associated article counts, is provided to offer insight into the scope of this screening. The total number of articles before and after this screening is also displayed.</li>\n",
    "</ul>\n",
    "<p>This dual screening process is designed to refine the dataset by focusing on articles that are both in English and relevant to the US context.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages to be removed that are not English or Missing:\n",
      "- German: 1\n",
      "- French: 1\n",
      "\n",
      "Total articles (Before Screen 1a): 10,272\n",
      "Total articles that are in the list of languages to be removed: 2\n",
      "Total articles (After Screen 1a): 10,270\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Screening 1a: Language (Non-English)\n",
    "######################################################\n",
    "\n",
    "# Replace NaN with a placeholder (\"Missing\") for easier filtering to ensure that NaNs are handled correctly.\n",
    "master_records['language'] = master_records['language'].fillna('Missing')\n",
    "\n",
    "# List of languages to remove (in this case, any language that is not 'English' or 'Missing')\n",
    "languages_to_remove = master_records['language'].unique()\n",
    "languages_to_remove = [lang for lang in languages_to_remove if lang not in ['Missing', 'English']]\n",
    "\n",
    "# Print the languages that will be removed along with the count of associated articles\n",
    "print(\"Languages to be removed that are not English or Missing:\")\n",
    "for language in languages_to_remove:\n",
    "    count = master_records[master_records['language'] == language].shape[0]\n",
    "    print(f\"- {language}: {count:,}\")\n",
    "\n",
    "# Total articles before removal\n",
    "total_before = master_records.shape[0]\n",
    "print(f\"\\nTotal articles (Before Screen 1a): {total_before:,}\")\n",
    "\n",
    "# Count the records that are in the list of languages to be removed\n",
    "num_records_to_remove = master_records['language'].isin(languages_to_remove).sum()\n",
    "print(f\"Total articles that are in the list of languages to be removed: {num_records_to_remove:,}\")\n",
    "\n",
    "# Remove the records that are not in English or Missing\n",
    "filtered_records = master_records[(master_records['language'] == 'Missing') | (master_records['language'] == 'English')]\n",
    "\n",
    "# Total articles after removal\n",
    "total_after = filtered_records.shape[0]\n",
    "print(f\"Total articles (After Screen 1a): {total_after:,}\")\n",
    "\n",
    "# Apply the filtering and update the master_records DataFrame\n",
    "master_records = filtered_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifiers to be removed with associated article counts:\n",
      "- Australia: 7 articles\n",
      "- Canada: 6 articles\n",
      "- Canada (Ottawa): 1 articles\n",
      "- Canada (Winnipeg): 1 articles\n",
      "- Germany: 1 articles\n",
      "- Guam: 1 articles\n",
      "- Guatemala: 1 articles\n",
      "- Guyana: 1 articles\n",
      "- Iran: 1 articles\n",
      "- Israel: 2 articles\n",
      "- Italy: 1 articles\n",
      "- Japan: 1 articles\n",
      "- South Africa: 3 articles\n",
      "- South Africa (Cape Town): 1 articles\n",
      "- Spain (Barcelona): 1 articles\n",
      "- Turkey: 2 articles\n",
      "- United Kingdom: 4 articles\n",
      "- United Kingdom (Aberdeen): 1 articles\n",
      "- United Kingdom (Birmingham): 1 articles\n",
      "- United Kingdom (England): 5 articles\n",
      "- United Kingdom (Great Britain): 3 articles\n",
      "- United Kingdom (Reading): 1 articles\n",
      "- USSR: 1 articles\n",
      "\n",
      "Total articles (Before Screen 1b): 10,270\n",
      "Total articles that are in the list of identifiers to be removed: 47\n",
      "Total articles (After Screen 1b): 10,223\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Screening 1b: Geography code (Non-US contexts)\n",
    "######################################################\n",
    "\n",
    "# List of geographical identifiers to remove\n",
    "identifiers_to_remove = [\n",
    "    'Australia', 'Canada', 'Canada (Ottawa)', 'Canada (Winnipeg)', 'Germany',\n",
    "    'Guam', 'Guatemala', 'Guyana', 'Iran', 'Israel', 'Italy', 'Japan',\n",
    "    'South Africa', 'South Africa (Cape Town)', 'Spain (Barcelona)', 'Turkey',\n",
    "    'United Kingdom', 'United Kingdom (Aberdeen)', 'United Kingdom (Birmingham)',\n",
    "    'United Kingdom (England)', 'United Kingdom (Great Britain)', 'United Kingdom (Reading)',\n",
    "    'USSR'\n",
    "]\n",
    "\n",
    "# Function to count and remove records based on exact matching\n",
    "def remove_geo_records(df, identifiers):\n",
    "    # Print the identifiers that will be removed along with the count of associated articles\n",
    "    print(\"Identifiers to be removed with associated article counts:\")\n",
    "    for identifier in identifiers:\n",
    "        count = df[df['identifiersgeo'] == identifier].shape[0]\n",
    "        print(f\"- {identifier}: {count:,} articles\")\n",
    "        \n",
    "    # Total articles before removal\n",
    "    total_before = df.shape[0]\n",
    "    print(f\"\\nTotal articles (Before Screen 1b): {total_before:,}\")\n",
    "\n",
    "    # Count the records that are in the list of identifiers (to be removed)\n",
    "    num_records_to_remove = df['identifiersgeo'].isin(identifiers).sum()\n",
    "    print(f\"Total articles that are in the list of identifiers to be removed: {num_records_to_remove:,}\")\n",
    "\n",
    "    # Remove the records\n",
    "    filtered_df = df[~df['identifiersgeo'].isin(identifiers)]\n",
    "\n",
    "    # Total articles after removal\n",
    "    total_after = filtered_df.shape[0]\n",
    "    print(f\"Total articles (After Screen 1b): {total_after:,}\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Apply the removal function to master_records\n",
    "master_records = remove_geo_records(master_records, identifiers_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screening 2: Screen based on Article Type\n",
    "\n",
    "This section of the Jupyter notebook performs a screening of articles based on their publication type. \n",
    "\n",
    "To narrow our search to empirical articles, we tried to determine how ERIC distinguishes between types of publications (e.g., opinion, research, evaluation). The response to our query indicated that ERIC does not have a metadata field that identifies the research design or type of study but does sort by their own distinct categories that fall under “Reports.” Reports are sorted by “General,” Descriptive,” “Evaluative,” “Research,” and “Research-practitioner Partnerships.” ERIC experts were unable to guarantee that there would be no empirical research in categories outside of “research,” but stated that it is a “reasonable assumption that Reports-Research contains the most appropriate content.” Thus, we excluded articles that fell outside of “Reports-Research.”\n",
    "\n",
    "After analyzing the results and communicating with staff at ERIC, we have chosen to limit our dataset to articles that meet the following criteria for publicationtype:\n",
    "<ul>\n",
    "    <li>Articles that are classified solely as \"Journal Articles\" (n=30).</li>\n",
    "    <li>Articles that are classified as both \"Journal Articles\" and \"Reports - Research.\"</li>\n",
    "</ul>\n",
    "\n",
    "<b>Article Type: Screening Process</b>\n",
    "<ul>\n",
    "    <li>Articles are first screened based on their publication type metadata. Only articles that either are solely categorized as \"Journal Articles\" or that include both \"Journal Articles\" and \"Reports - Research\" are retained. Any article that combines \"Journal Articles\" with other types, but does not include \"Reports - Research,\" is filtered out during this step.</li>\n",
    "    <li>The count of articles for each publication type is displayed before the screening begins, with those meeting the criteria highlighted with a special marker.</li>\n",
    "    <li>After the screening, the total number of articles that have been filtered out and the total number of articles remaining are displayed.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication types and associated article counts:\n",
      "*Journal Articles: 10,223 articles\n",
      "*Reports - Research: 6,113 articles\n",
      "Reports - Descriptive: 2,026 articles\n",
      "Reports - Evaluative: 1,335 articles\n",
      "Opinion Papers: 533 articles\n",
      "Information Analyses: 418 articles\n",
      "Tests/Questionnaires: 330 articles\n",
      "Guides - Non-Classroom: 103 articles\n",
      "Guides - Classroom - Teacher: 62 articles\n",
      "Speeches/Meeting Papers: 31 articles\n",
      "Reports - General: 29 articles\n",
      "Legal/Legislative/Regulatory Materials: 22 articles\n",
      "Numerical/Quantitative Data: 17 articles\n",
      "Historical Materials: 17 articles\n",
      "Reference Materials - Bibliographies: 7 articles\n",
      "ERIC Publications: 5 articles\n",
      "Collected Works - Serials: 4 articles\n",
      "Collected Works - General: 3 articles\n",
      "Guides - General: 2 articles\n",
      "Book/Product Reviews: 2 articles\n",
      "Reference Materials - General: 1 articles\n",
      "Collected Works - Serial: 1 articles\n",
      "Collected Works - Proceedings: 1 articles\n",
      "\n",
      "Total articles (Before Screening 2): 10,223\n",
      "Total articles filtered out: 4,080\n",
      "Total articles (After Screening 2): 6,143\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Screening 2: Publication Type (Must be \"Journal Articles\" alone or \"Journal Articles\" with \"Reports - Research\")\n",
    "######################################################\n",
    "\n",
    "# Define the publication types required\n",
    "required_types = [\"Journal Articles\", \"Reports - Research\"]\n",
    "\n",
    "# Function to filter based on publication types\n",
    "def filter_publication_type(df, required_types):\n",
    "    # Print the publication types and associated article counts\n",
    "    print(\"Publication types and associated article counts:\")\n",
    "    \n",
    "    # Split the 'publicationtype' field on commas, then explode the list into separate rows\n",
    "    # This allows counting each publication type individually, even when multiple types are listed in a single row\n",
    "    publication_counts = df['publicationtype'].str.split(', ').explode().value_counts()\n",
    "\n",
    "    # Iterate over each unique publication type and its associated count\n",
    "    for p_type, count in publication_counts.items():\n",
    "        # Check if the publication type is one of the required types to keep\n",
    "        marker = \"*\" if p_type in required_types else \"\"\n",
    "        \n",
    "        # Print the publication type, the count of articles, and add a \"*\" marker if it is a required type\n",
    "        print(f\"{marker}{p_type}: {count:,} articles\")\n",
    "\n",
    "    # Total articles before removal\n",
    "    total_before = df.shape[0]\n",
    "    print(f\"\\nTotal articles (Before Screening 2): {total_before:,}\")\n",
    "\n",
    "    # Filter the records:\n",
    "    # - Keep records where \"Journal Articles\" is the only type\n",
    "    # - OR keep records where \"Journal Articles\" AND \"Reports - Research\" are present\n",
    "    filtered_df = df[df['publicationtype'].apply(lambda x: x == \"Journal Articles\" or \n",
    "                                                 (\"Journal Articles\" in x and \"Reports - Research\" in x))]\n",
    "\n",
    "    # Total articles filtered out\n",
    "    total_filtered_out = total_before - filtered_df.shape[0]\n",
    "    print(f\"Total articles filtered out: {total_filtered_out:,}\")\n",
    "\n",
    "    # Total articles after removal\n",
    "    total_after = filtered_df.shape[0]\n",
    "    print(f\"Total articles (After Screening 2): {total_after:,}\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Apply the filter function to master_records\n",
    "master_records = filter_publication_type(master_records, required_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screening 3: Journal Screen\n",
    "\n",
    "This section of the Jupyter notebook performs a screening of articles based on the journals in which they were published. Phase II included a journal screening process of 1,508 distinct journals. This screening process included reviewing each journal to determine whether the journal conducted peer review or was publishing empirical articles through an analysis of ERIC's descriptors and internet searches. If the journal did not conduct peer review or was primarily for commentary or opinion pieces, the journal was excluded. If the journal was in a language other than English, the journal was excluded. We also checked whether the articles included in that journal were based in the US, empirical, or relevant  using a pivot table of the output. If none of the articles met the criteria, the journal was also excluded.\n",
    "\n",
    "<b>Journal Source: Screening Process</b>\n",
    "<ul>\n",
    "    <li>Articles are screened based on the journal in which they were published. The names of the journals in the dataset are matched with the screening list created during the journal review process.</li>\n",
    "    <li>To ensure accurate matching, all journal names in both the dataset and the screening list are converted to lowercase. This accounts for any differences in letter casing that could prevent a match.</li>\n",
    "    <li>The screening process checks how many journal names in the dataset match those in the screening list and how many do not. A report is generated to show the number of matches and non-matches.</li>\n",
    "    <li>Non-matching sources are identified and reviewed. A list of these sources, along with the number of associated articles, is provided for further analysis.</li>\n",
    "    <li>After the screening is complete, the total number of articles that have been filtered out and the total number remaining are displayed. A summary is provided, showing the breakdown of the exclusion types.</li>\n",
    "</ul>\n",
    "\n",
    "This screening process ensures that only articles from relevant journals are retained in the dataset. Journals that do not meet the inclusion criteria or that contain irrelevant articles are systematically excluded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mMerge Summary\u001b[0m\n",
      "Merge successful of source (journal) from master and screen data frames\n",
      "  Total matches in merge: 6,143\n",
      "  Total non-matches in merge: 0\n",
      "\n",
      "\u001b[1mSummary of Screening 3\u001b[0m\n",
      "Total: 1,507 journals | 6,143 articles\n",
      "  Included Total: 629 journals | 4,306 articles\n",
      "  Excluded Total: 878 journals | 1,837 articles\n",
      "   - Exclude(Article Not Relevant): 576 journals | 1,294 articles\n",
      "   - Exclude: 287 journals | 519 articles\n",
      "   - Exclude(Non-US Journal Context): 9 journals | 24 articles\n",
      "   - Exclude(Reference): 5 journals | 0 articles\n",
      "   - Exclude(Literature Review): 1 journals | 0 articles\n",
      "\n",
      "Total articles (Before Screening 3): 6,143\n",
      "Total articles filtered out (Screening 3): 1,837\n",
      "Total articles (After Screening 3): 4,306\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Screening 3: Journal Screen\n",
    "######################################################\n",
    "\n",
    "# URL of the Journal Screen Excel file from your GitHub repository\n",
    "file_url = 'https://raw.githubusercontent.com/s-baser/P20Tracking_Review/main/Journal_Screen.xlsx'\n",
    "\n",
    "# Load the entire sheet from the Journal Screen Excel file\n",
    "df_full_sheet = pd.read_excel(file_url, sheet_name='source_screen')\n",
    "\n",
    "# Slice the DataFrame to extract the table (ERIC_screen) from A1 to B1509, ensuring proper headers\n",
    "df_screening = df_full_sheet.iloc[1:1509, 0:2]  # Rows 1 to 1508 (1509 rows), Columns A (0) and B (1)\n",
    "\n",
    "# Rename the columns to reflect the proper header names\n",
    "df_screening.columns = ['source', 'source_screen']\n",
    "\n",
    "# Convert 'source' columns in both DataFrames to lowercase to ensure case-insensitive matching\n",
    "df_screening['source'] = df_screening['source'].str.lower()\n",
    "master_records['source'] = master_records['source'].str.lower()\n",
    "\n",
    "# Total articles before removal in Screening 3\n",
    "total_before_screening_3 = master_records.shape[0]\n",
    "\n",
    "# Merge the screening data into the main data (master_records) on the 'source' column\n",
    "master_records = pd.merge(master_records, df_screening, how='left', on='source')\n",
    "\n",
    "# Check for matches and non-matches\n",
    "matches = master_records['source_screen'].notna().sum()\n",
    "non_matches = master_records['source_screen'].isna().sum()\n",
    "\n",
    "# Print merge summary\n",
    "print(\"\\n\\033[1mMerge Summary\\033[0m\")\n",
    "print(\"Merge successful of source (journal) from master and screen data frames\")\n",
    "print(f\"  Total matches in merge: {matches:,}\")\n",
    "print(f\"  Total non-matches in merge: {non_matches:,}\")\n",
    "\n",
    "# Filter out the articles that need to be excluded based on the 'source_screen' column\n",
    "master_records_filtered = master_records[~master_records['source_screen'].str.startswith('Exclude', na=False)]\n",
    "\n",
    "# Total articles filtered out in Screening 3\n",
    "total_filtered_out_screening_3 = total_before_screening_3 - master_records_filtered.shape[0]\n",
    "\n",
    "# Total articles after removal in Screening 3\n",
    "total_after_screening_3 = master_records_filtered.shape[0]\n",
    "\n",
    "# Summary of Screening 3\n",
    "print(\"\\n\\033[1mSummary of Screening 3\\033[0m\")\n",
    "print(f\"Total: {df_screening['source'].nunique():,} journals | {total_before_screening_3:,} articles\")\n",
    "print(f\"  Included Total: {df_screening[df_screening['source_screen'].str.startswith('Include', na=False)]['source'].nunique()} journals | {total_after_screening_3:,} articles\")\n",
    "print(f\"  Excluded Total: {df_screening[df_screening['source_screen'].str.startswith('Exclude', na=False)]['source'].nunique()} journals | {total_filtered_out_screening_3:,} articles\")\n",
    "\n",
    "# Breakdown of Exclude Types\n",
    "exclude_types_updated = df_screening[df_screening['source_screen'].str.startswith('Exclude')]['source_screen'].value_counts()\n",
    "for exclude_type, count in exclude_types_updated.items():\n",
    "    count_articles = master_records[master_records['source_screen'] == exclude_type].shape[0]\n",
    "    print(f\"   - {exclude_type}: {count} journals | {count_articles:,} articles\")\n",
    "\n",
    "# Summary of totals\n",
    "print(f\"\\nTotal articles (Before Screening 3): {total_before_screening_3:,}\")\n",
    "print(f\"Total articles filtered out (Screening 3): {total_filtered_out_screening_3:,}\")\n",
    "print(f\"Total articles (After Screening 3): {total_after_screening_3:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Articles in Each Search (source_search):\n",
      "Search 1: 1,724 articles\n",
      "Search 2: 2,280 articles\n",
      "Search 3: 2,134 articles\n",
      "Search 4: 5 articles\n"
     ]
    }
   ],
   "source": [
    "# Count the number of articles for each unique value in the 'source_search' column\n",
    "articles_per_search = master_records['source_search'].value_counts()\n",
    "\n",
    "# Print the number of articles for each search in 'source_search'\n",
    "sorted_searches = articles_per_search.loc[['Search 1', 'Search 2', 'Search 3', 'Search 4']]\n",
    "print(\"\\nNumber of Articles in Each Search (source_search):\")\n",
    "for search, count in sorted_searches.items():\n",
    "    print(f\"{search}: {count:,} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screening 4: Less Relevant Subjects Screen\n",
    "\n",
    "This section of the Jupyter notebook performs a screening for subjects not relevant to tracking (e.g., eye tracking). The screening identified obvious examples of unrelated subjects and used pivot tables for quick validation. The majority of removed articles originated from search 2.\n",
    "\n",
    "<b>Subject Source: Screening Process</b>\n",
    "<ul>\n",
    "    <li> Used Excel pivot tables to analyze unique subject counts for each article and identify subjects for removal.\n",
    "    <li> Removed 470 articles through the screening process.</li>\n",
    "</ul>\n",
    "\n",
    "This screening process ensures that only articles with relevant subjects are retained in the dataset. Subjects that do not meet the inclusion criteria or are irrelevant are systematically excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of records removed for each less relevant subject:\n",
      "anxiety: 42\n",
      "athletes: 5\n",
      "audiovisual communications: 1\n",
      "client characteristics (human services): 1\n",
      "cognitive ability: 42\n",
      "cognitive processes: 89\n",
      "dentistry: 2\n",
      "eye movements: 121\n",
      "faculty development: 90\n",
      "family life: 3\n",
      "food service: 1\n",
      "foods instruction: 1\n",
      "handwriting: 3\n",
      "muscular strength: 1\n",
      "patients: 5\n",
      "perinatal influences: 1\n",
      "premature infants: 1\n",
      "psychiatric services: 1\n",
      "psychiatry: 7\n",
      "school space: 1\n",
      "smoking: 5\n",
      "video technology: 44\n",
      "workstations: 1\n",
      "\n",
      "\u001b[1mSummary of Screening 4\u001b[0m\n",
      "Total articles before Screening 4: 4,306\n",
      "Total articles removed (Screening 4): 468\n",
      "Total articles after Screening 4: 3,909\n",
      "\n",
      "\u001b[1mNumber of Articles in Each Search (source_search):\u001b[0m\n",
      "Search 1: 1,724 articles\n",
      "Search 2: 2,280 articles\n",
      "Search 3: 2,134 articles\n",
      "Search 4: 5 articles\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Screening 4: Subject Filtering\n",
    "######################################################\n",
    "\n",
    "# Define the subjects from both lists that are considered less relevant, now alphabetized.\n",
    "less_relevant_subjects = sorted([\n",
    "    'anxiety', 'athletes', 'audiovisual communications', 'client characteristics (human services)',\n",
    "    'cognitive ability', 'cognitive processes', 'dentistry', 'eye movements', 'faculty development',\n",
    "    'family life', 'food service', 'foods instruction', 'handwriting', 'muscular strength',\n",
    "    'patients', 'perinatal influences', 'premature infants', 'psychiatric services', \n",
    "    'psychiatry', 'school space', 'smoking', 'video technology', 'workstations'\n",
    "])\n",
    "\n",
    "# Split the 'subject' column into individual subjects, explode them into separate rows, and strip/clean the data.\n",
    "exploded_subjects = master_records_filtered['subject'].str.split(',').explode().str.strip().str.lower()\n",
    "\n",
    "# Create a boolean mask for subjects to be removed\n",
    "mask = exploded_subjects.isin(less_relevant_subjects)\n",
    "\n",
    "# Filter out rows where the subject is in the less relevant subjects, but we need to use the original index\n",
    "filtered_master_records = master_records_filtered[~master_records_filtered.index.isin(exploded_subjects[mask].index)]\n",
    "\n",
    "# Create a dictionary to store counts of each removed subject\n",
    "removed_subjects_count = {subject: exploded_subjects[exploded_subjects == subject].count() for subject in less_relevant_subjects}\n",
    "\n",
    "# Count how many articles were removed for each subject and the total removed\n",
    "total_removed_articles = sum(removed_subjects_count.values())\n",
    "\n",
    "# Print the counts for each less relevant subject\n",
    "print(\"\\nNumber of records removed for each less relevant subject:\")\n",
    "for subject, count in removed_subjects_count.items():\n",
    "    print(f\"{subject}: {count}\")\n",
    "\n",
    "# Summary of Screening 4\n",
    "print(\"\\n\\033[1mSummary of Screening 4\\033[0m\")\n",
    "print(f\"Total articles before Screening 4: {len(master_records_filtered):,}\")\n",
    "print(f\"Total articles removed (Screening 4): {total_removed_articles:,}\")\n",
    "print(f\"Total articles after Screening 4: {len(filtered_master_records):,}\")\n",
    "\n",
    "# Count the number of articles for each unique value in the 'source_search' column\n",
    "articles_per_search = master_records['source_search'].value_counts()\n",
    "\n",
    "# Print the number of articles for each search in 'source_search'\n",
    "sorted_searches = articles_per_search.loc[['Search 1', 'Search 2', 'Search 3', 'Search 4']]\n",
    "print(\"\\n\\033[1mNumber of Articles in Each Search (source_search):\\033[0m\")\n",
    "for search, count in sorted_searches.items():\n",
    "    print(f\"{search}: {count:,} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screening 5: Publication Type Screen (#2)\n",
    "\n",
    "This section of the Jupyter notebook refines the dataset by performing an additional screening based on publication type, following a prior round of subject-based analysis.\n",
    "\n",
    "To ensure the dataset remains focused on relevant, empirical research, we further screen out articles categorized under non-research publication types. Specifically, we remove articles labeled with types such as \"Opinion Papers\" or \"Guides\" that may still be tagged alongside valid types like \"Journal Articles\" or \"Reports - Research.\"\n",
    "\n",
    "<b>Publication Type: Screening Process</b>\n",
    "<ul> \n",
    "    <li>Articles classified under non-research types such as \"Opinion Papers,\" \"Guides - Non-Classroom,\" \"Speeches/Meeting Papers,\" and similar categories are removed, even if they also include \"Journal Articles\" or \"Reports - Research.\"</li> \n",
    "    <li>A count of articles for each publication type is displayed before screening begins, with non-research types highlighted for removal.</li> \n",
    "    <li>After the screening, 26 articles removed.</li> \n",
    "</ul>\n",
    "\n",
    "This additional filtering ensures that only articles with relevant research types remain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication types and associated article counts before screening:\n",
      "Journal Articles: 3,909 articles\n",
      "Reports - Research: 3,886 articles\n",
      "Tests/Questionnaires: 150 articles\n",
      "Information Analyses: 75 articles\n",
      "*Opinion Papers: 11 articles\n",
      "*Speeches/Meeting Papers: 10 articles\n",
      "Numerical/Quantitative Data: 8 articles\n",
      "*Guides - Non-Classroom: 3 articles\n",
      "Reports - Descriptive: 3 articles\n",
      "Reports - Evaluative: 3 articles\n",
      "*Guides - Classroom - Teacher: 2 articles\n",
      "Reports - General: 1 articles\n",
      "Historical Materials: 1 articles\n",
      "\n",
      "\u001b[1mSummary of Screening 5\u001b[0m\n",
      "Total articles (Before Screening 5): 3,909\n",
      "Total articles filtered out: 26\n",
      "Total articles (After Screening 5): 3,883\n",
      "\n",
      "\u001b[1mNumber of Articles in Each Search (source_search):\u001b[0m\n",
      "Search 1: 1,370 articles\n",
      "Search 2: 1,472 articles\n",
      "Search 3: 1,037 articles\n",
      "Search 4: 4 articles\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Screening 5: Remove Specific Publication Types\n",
    "######################################################\n",
    "\n",
    "# Define the publication types to be removed\n",
    "remove_types = [\n",
    "    \"Opinion Papers\", \"Guides - Non-Classroom\", \"Guides - Classroom - Teacher\", \n",
    "    \"Speeches/Meeting Papers\", \"Legal/Legislative/Regulatory Materials\", \n",
    "    \"Reference Materials - Bibliographies\", \"Guides - General\"\n",
    "]\n",
    "\n",
    "# Function to filter out unwanted publication types\n",
    "def filter_out_unwanted_types(df, remove_types):\n",
    "    # Print the publication types and associated article counts\n",
    "    print(\"Publication types and associated article counts before screening:\")\n",
    "\n",
    "    # Split the 'publicationtype' field on commas, then explode the list into separate rows\n",
    "    # This allows counting each publication type individually, even when multiple types are listed in a single row\n",
    "    publication_counts = df['publicationtype'].str.split(', ').explode().value_counts()\n",
    "\n",
    "    # Iterate over each unique publication type and its associated count\n",
    "    for p_type, count in publication_counts.items():\n",
    "        marker = \"*\" if p_type in remove_types else \"\"\n",
    "        print(f\"{marker}{p_type}: {count:,} articles\")\n",
    "\n",
    "    \n",
    "    # Total articles before removal\n",
    "    total_before = df.shape[0]\n",
    "    print(\"\\n\\033[1mSummary of Screening 5\\033[0m\")\n",
    "    print(f\"Total articles (Before Screening 5): {total_before:,}\")\n",
    "\n",
    "    # Filter the records:\n",
    "    # - Remove any record where at least one of the remove_types is present in the 'publicationtype' field\n",
    "    filtered_df = df[~df['publicationtype'].apply(lambda x: any(r_type in x for r_type in remove_types))]\n",
    "\n",
    "    # Total articles filtered out\n",
    "    total_filtered_out = total_before - filtered_df.shape[0]\n",
    "    print(f\"Total articles filtered out: {total_filtered_out:,}\")\n",
    "\n",
    "    # Total articles after removal\n",
    "    total_after = filtered_df.shape[0]\n",
    "    print(f\"Total articles (After Screening 5): {total_after:,}\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Apply the Screening 5 filter function to the results from Screening 4\n",
    "final_filtered_records = filter_out_unwanted_types(filtered_master_records, remove_types)\n",
    "\n",
    "\n",
    "# Count the number of articles for each unique value in the 'source_search' column\n",
    "articles_per_search = final_filtered_records['source_search'].value_counts()\n",
    "\n",
    "# Print the number of articles for each search in 'source_search'\n",
    "sorted_searches = articles_per_search.loc[['Search 1', 'Search 2', 'Search 3', 'Search 4']]\n",
    "print(\"\\n\\033[1mNumber of Articles in Each Search (source_search):\\033[0m\")\n",
    "for search, count in sorted_searches.items():\n",
    "    print(f\"{search}: {count:,} articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## <ins>Saving the output</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Searches to CSV\n",
    "This section of the notebook details the process of exporting our fetched ERIC database records from our four searches into a single CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication types and associated article counts before screening:\n",
      "Journal Articles: 3,927 articles\n",
      "Reports - Research: 3,904 articles\n",
      "Tests/Questionnaires: 151 articles\n",
      "Information Analyses: 75 articles\n",
      "*Opinion Papers: 11 articles\n",
      "*Speeches/Meeting Papers: 10 articles\n",
      "Numerical/Quantitative Data: 8 articles\n",
      "*Guides - Non-Classroom: 3 articles\n",
      "Reports - Descriptive: 3 articles\n",
      "Reports - Evaluative: 3 articles\n",
      "*Guides - Classroom - Teacher: 2 articles\n",
      "Reports - General: 1 articles\n",
      "Historical Materials: 1 articles\n",
      "\n",
      "\u001b[1mSummary of Screening 5\u001b[0m\n",
      "Total articles (Before Screening 5): 3,927\n",
      "Total articles filtered out: 26\n",
      "Total articles (After Screening 5): 3,901\n",
      "Data exported successfully to C:\\Users\\sbaser\\OneDrive - University of Georgia\\Shared Folders\\P-20 Parallels and Perils\\Data Collection\\Phase 2 - Literature Review Search\\ERIC API Resources\\ERIC Searches\\Tracking_API_Output_Search2d_20240906_083927.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the path and filename with the current date and time\n",
    "base_path = \"C:\\\\Users\\\\sbaser\\\\OneDrive - University of Georgia\\\\Shared Folders\\\\P-20 Parallels and Perils\\\\Data Collection\\\\Phase 2 - Literature Review Search\\\\ERIC API Resources\\\\ERIC Searches\\\\\"\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"Tracking_API_Output_Search2d_{current_time}.csv\"\n",
    "full_path = os.path.join(base_path, filename)\n",
    "\n",
    "# Apply the Screening 5 filter function to the results from Screening 4\n",
    "final_filtered_records = filter_out_unwanted_types(filtered_master_records, remove_types)\n",
    "\n",
    "# Export the final_filtered_records DataFrame to a CSV file\n",
    "final_filtered_records.to_csv(full_path, index=False)\n",
    "\n",
    "# Print a message confirming the export\n",
    "print(f\"Data exported successfully to {full_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
